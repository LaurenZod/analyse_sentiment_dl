name: CI – Train changed models

on:
  push:
    paths:
      - "scripts/run_bert.py"
      - "scripts/run_lstm_torch.py"
      - "scripts/run_textcnn.py"
      - "scripts/run_logreg.py"
      - "requirements.txt"
      - "requirements_tf.txt"
  workflow_dispatch: {}

jobs:
  # Détection des fichiers modifiés
  detect-changes:
    runs-on: ubuntu-latest
    outputs:
      bert:     ${{ steps.filter.outputs.bert }}
      lstm:     ${{ steps.filter.outputs.lstm }}
      textcnn:  ${{ steps.filter.outputs.textcnn }}
      logreg:   ${{ steps.filter.outputs.logreg }}
      common:   ${{ steps.filter.outputs.common }}
    steps:
      - uses: actions/checkout@v4
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            bert:
              - 'scripts/run_bert.py'
            lstm:
              - 'scripts/run_lstm_torch.py'
            textcnn:
              - 'scripts/run_textcnn.py'
              - 'requirements_tf.txt'
            logreg:
              - 'scripts/run_logreg.py'
            common:
              - 'requirements.txt'

  # --------------------------
  # BERT (PyTorch)
  # --------------------------
  train-bert:
    needs: detect-changes
    if: needs.detect-changes.outputs.bert == 'true' || needs.detect-changes.outputs.common == 'true'
    runs-on: ubuntu-latest
    env:
      # MLflow à partir du secret du repo
      MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
      # HF en mode PyTorch only
      TRANSFORMERS_NO_TF: "1"
      TOKENIZERS_PARALLELISM: "false"
      # Dataset
      DATA_CSV: data/training.csv
      DATA_CSV_URL: ${{ secrets.DATA_CSV_URL }} # optionnel
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
      - name: Install deps
        run: |
          python -m pip install -U pip
          pip install -r requirements.txt
          pip install accelerate
      - name: Prepare dataset
        run: |
          mkdir -p data
          if [ ! -f "$DATA_CSV" ]; then
            if [ -n "$DATA_CSV_URL" ]; then
              echo "Downloading dataset from \$DATA_CSV_URL -> \$DATA_CSV"
              curl -L "$DATA_CSV_URL" -o "$DATA_CSV"
            else
              echo "Dataset not found and DATA_CSV_URL not set. Skipping job."
              exit 0
            fi
          fi
      - name: Train (smoketest)
        run: |
          python -m scripts.run_bert \
            --data "$DATA_CSV" \
            --subset_rows 5000 \
            --model_name "distilbert-base-uncased" \
            --epochs 1 \
            --batch_size 32 \
            --max_length 96 \
            --lr 3e-5

  # --------------------------
  # LSTM (PyTorch)
  # --------------------------
  train-lstm:
    needs: detect-changes
    if: needs.detect-changes.outputs.lstm == 'true' || needs.detect-changes.outputs.common == 'true'
    runs-on: ubuntu-latest
    env:
      MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
      DATA_CSV: data/training.csv
      DATA_CSV_URL: ${{ secrets.DATA_CSV_URL }}     # optionnel
      EMB_PATH: data/embeddings/glove.twitter.27B.200d.txt
      GLOVE_200D_URL: ${{ secrets.GLOVE_200D_URL }} # optionnel
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
      - name: Install deps
        run: |
          python -m pip install -U pip
          pip install -r requirements.txt
      - name: Prepare data & embeddings
        run: |
          mkdir -p data data/embeddings
          # Dataset
          if [ ! -f "$DATA_CSV" ]; then
            if [ -n "$DATA_CSV_URL" ]; then
              echo "Downloading dataset from \$DATA_CSV_URL -> \$DATA_CSV"
              curl -L "$DATA_CSV_URL" -o "$DATA_CSV"
            else
              echo "No dataset. Skipping job."
              exit 0
            fi
          fi
          # Embeddings
          if [ ! -f "$EMB_PATH" ]; then
            if [ -n "$GLOVE_200D_URL" ]; then
              echo "Downloading GloVe 200d from \$GLOVE_200D_URL -> \$EMB_PATH"
              mkdir -p "$(dirname "$EMB_PATH")"
              curl -L "$GLOVE_200D_URL" -o "$EMB_PATH"
            else
              echo "No embeddings. Skipping job."
              exit 0
            fi
          fi
      - name: Train (smoketest)
        run: |
          python -m scripts.run_lstm_torch \
            --data "$DATA_CSV" \
            --embedding_path "$EMB_PATH" \
            --embedding_dim 200 \
            --subset_rows 10000 \
            --epochs 2 \
            --batch_size 256

  # --------------------------
  # TextCNN (TensorFlow/Keras) – requirements_tf.txt
  # --------------------------
  train-textcnn:
    needs: detect-changes
    if: needs.detect-changes.outputs.textcnn == 'true' || needs.detect-changes.outputs.common == 'true'
    runs-on: ubuntu-latest
    env:
      MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
      DATA_CSV: data/training.csv
      DATA_CSV_URL: ${{ secrets.DATA_CSV_URL }}     # optionnel
      EMB_PATH: data/embeddings/glove.twitter.27B.200d.txt
      GLOVE_200D_URL: ${{ secrets.GLOVE_200D_URL }} # optionnel
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-tf-${{ hashFiles('requirements_tf.txt') }}
      - name: Install deps (TF stack)
        run: |
          python -m pip install -U pip
          pip install -r requirements_tf.txt
      - name: Prepare data & embeddings
        run: |
          mkdir -p data data/embeddings
          # Dataset
          if [ ! -f "$DATA_CSV" ]; then
            if [ -n "$DATA_CSV_URL" ]; then
              echo "Downloading dataset from \$DATA_CSV_URL -> \$DATA_CSV"
              curl -L "$DATA_CSV_URL" -o "$DATA_CSV"
            else
              echo "No dataset. Skipping job."
              exit 0
            fi
          fi
          # Embeddings
          if [ ! -f "$EMB_PATH" ]; then
            if [ -n "$GLOVE_200D_URL" ]; then
              echo "Downloading GloVe 200d from \$GLOVE_200D_URL -> \$EMB_PATH"
              mkdir -p "$(dirname "$EMB_PATH")"
              curl -L "$GLOVE_200D_URL" -o "$EMB_PATH"
            else
              echo "No embeddings. Skipping job."
              exit 0
            fi
          fi
      - name: Train (smoketest)
        run: |
          python -m scripts.run_textcnn \
            --data "$DATA_CSV" \
            --embedding_path "$EMB_PATH" \
            --embedding_dim 200 \
            --subset_rows 5000 \
            --epochs 2 \
            --batch_size 256

  # --------------------------
  # Baseline LogReg
  # --------------------------
  train-logreg:
    needs: detect-changes
    if: needs.detect-changes.outputs.logreg == 'true' || needs.detect-changes.outputs.common == 'true'
    runs-on: ubuntu-latest
    env:
      MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
      DATA_CSV: data/training.csv
      DATA_CSV_URL: ${{ secrets.DATA_CSV_URL }} # optionnel
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
      - name: Install deps
        run: |
          python -m pip install -U pip
          pip install -r requirements.txt
      - name: Prepare dataset
        run: |
          mkdir -p data
          if [ ! -f "$DATA_CSV" ]; then
            if [ -n "$DATA_CSV_URL" ]; then
              echo "Downloading dataset from \$DATA_CSV_URL -> \$DATA_CSV"
              curl -L "$DATA_CSV_URL" -o "$DATA_CSV"
            else
              echo "No dataset. Skipping job."
              exit 0
            fi
          fi
      - name: Train (fast)
        run: |
          python -m scripts.run_logreg \
            --data "$DATA_CSV" \
            --subset_rows 20000 \
            --ngram_max 2