name: CI ‚Äì Train changed models

on:
  push:
    paths:
      - "scripts/run_bert.py"
      - "scripts/run_lstm_torch.py"
      - "scripts/run_textcnn.py"
      - "scripts/run_logreg.py"
      - "requirements.txt"
  workflow_dispatch: {}

jobs:
  # D√©tection des fichiers modifi√©s
  detect-changes:
    runs-on: ubuntu-latest
    outputs:
      bert:     ${{ steps.filter.outputs.bert }}
      lstm:     ${{ steps.filter.outputs.lstm }}
      textcnn:  ${{ steps.filter.outputs.textcnn }}
      logreg:   ${{ steps.filter.outputs.logreg }}
      common:   ${{ steps.filter.outputs.common }}
    steps:
      - uses: actions/checkout@v4
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            bert:
              - 'scripts/run_bert.py'
            lstm:
              - 'scripts/run_lstm_torch.py'
            textcnn:
              - 'scripts/run_textcnn.py'
            logreg:
              - 'scripts/run_logreg.py'
            common:
              - 'requirements.txt'

  # --------------------------
  # BERT (PyTorch)
  # --------------------------
  train-bert:
    needs: detect-changes
    if: needs.detect-changes.outputs.bert == 'true' || needs.detect-changes.outputs.common == 'true'
    runs-on: ubuntu-latest
    env:
      MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
      TRANSFORMERS_NO_TF: "1"
      TOKENIZERS_PARALLELISM: "false"
      DATA_CSV: data/training.csv
      DATA_CSV_URL: ${{ secrets.DATA_CSV_URL }} # format s3://...
    steps:
      - uses: actions/checkout@v4
      # üîë Configuration des identifiants AWS pour l'acc√®s S3
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: eu-north-1 # R√©gion du bucket S3
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
      - name: Install deps & AWS CLI
        run: |
          python -m pip install -U pip
          pip install -r requirements.txt
          pip install accelerate
          pip install awscli # üõ†Ô∏è Installation de l'AWS CLI
      
      - name: ‚öôÔ∏è V√©rifier la connexion MLflow
        run: python validate_mlflow.py
        
      - name: Prepare dataset (via AWS CLI)
        run: |
          mkdir -p data
          if [ ! -f "$DATA_CSV" ]; then
            if [ -n "$DATA_CSV_URL" ]; then
              echo "Downloading dataset from $DATA_CSV_URL -> $DATA_CSV"
              # üõ†Ô∏è Remplacement de curl par aws s3 cp
              aws s3 cp "$DATA_CSV_URL" "$DATA_CSV" --region eu-north-1 
            else
              echo "Dataset not found and DATA_CSV_URL not set. Skipping job."
              exit 0
            fi
          fi
      - name: Valider le Code et l'Environnement (Smoketest)
        run: |
          python -m scripts.run_bert \
            --data "$DATA_CSV" \
            --subset_rows 100 \
            --model_name "distilbert-base-uncased" \
            --epochs 1 \
            --batch_size 32 \
            --max_length 96 \
            --lr 3e-5

  # --------------------------
  # LSTM (PyTorch)
  # --------------------------
  train-lstm:
    needs: detect-changes
    if: needs.detect-changes.outputs.lstm == 'true' || needs.detect-changes.outputs.common == 'true'
    runs-on: ubuntu-latest
    env:
      MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
      DATA_CSV: data/training.csv
      DATA_CSV_URL: ${{ secrets.DATA_CSV_URL }}     # format s3://...
      EMB_PATH: data/embeddings/glove.twitter.27B.200d.txt
      GLOVE_200D_URL: ${{ secrets.GLOVE_200D_URL }} # format s3://...
    steps:
      - uses: actions/checkout@v4
      # üîë Configuration des identifiants AWS pour l'acc√®s S3
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: eu-north-1 # R√©gion du bucket S3
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
      - name: Install deps & AWS CLI
        run: |
          python -m pip install -U pip
          pip install -r requirements.txt
          pip install awscli # üõ†Ô∏è Installation de l'AWS CLI
          
      - name: ‚öôÔ∏è V√©rifier la connexion MLflow
        run: python validate_mlflow.py
        
      - name: Prepare data & embeddings (via AWS CLI)
        run: |
          mkdir -p data data/embeddings
          # Dataset
          if [ ! -f "$DATA_CSV" ]; then
            if [ -n "$DATA_CSV_URL" ]; then
              echo "Downloading dataset from $DATA_CSV_URL -> $DATA_CSV"
              # üõ†Ô∏è Remplacement de curl par aws s3 cp
              aws s3 cp "$DATA_CSV_URL" "$DATA_CSV" --region eu-north-1
            else
              echo "No dataset. Skipping job."
              exit 0
            fi
          fi
          # Embeddings
          if [ ! -f "$EMB_PATH" ]; then
            if [ -n "$GLOVE_200D_URL" ]; then
              echo "Downloading GloVe 200d from $GLOVE_200D_URL -> $EMB_PATH"
              mkdir -p "$(dirname "$EMB_PATH")"
              # üõ†Ô∏è Remplacement de curl par aws s3 cp
              aws s3 cp "$GLOVE_200D_URL" "$EMB_PATH" --region eu-north-1
            else
              echo "No embeddings. Skipping job."
              exit 0
            fi
          fi
      - name: Train (smoketest)
        run: |
          python -m scripts.run_lstm_torch \
            --data "$DATA_CSV" \
            --embedding_path "$EMB_PATH" \
            --embedding_dim 200 \
            --subset_rows 10000 \
            --epochs 2 \
            --batch_size 256

  # --------------------------
  # TextCNN (TensorFlow/Keras)
  # --------------------------
  train-textcnn:
    needs: detect-changes
    if: needs.detect-changes.outputs.textcnn == 'true' || needs.detect-changes.outputs.common == 'true'
    runs-on: ubuntu-latest
    env:
      MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
      DATA_CSV: data/training.csv
      DATA_CSV_URL: ${{ secrets.DATA_CSV_URL }}     # format s3://...
      EMB_PATH: data/embeddings/glove.twitter.27B.200d.txt
      GLOVE_200D_URL: ${{ secrets.GLOVE_200D_URL }} # format s3://...
    steps:
      - uses: actions/checkout@v4
      # üîë Configuration des identifiants AWS pour l'acc√®s S3
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: eu-north-1 # R√©gion du bucket S3
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-textcnn-ci-${{ hashFiles('requirements_textcnn.txt') }} 
          restore-keys: |
            ${{ runner.os }}-pip-textcnn-ci- 
      - name: Install deps & AWS CLI (TF stack)
        run: |
          python -m pip install -U pip
          # ‚ö†Ô∏è CORRECTION N√âCESSAIRE: Vous devez avoir mis √† jour requirements_textcnn.txt pour TF 2.15.0
          pip install -r requirements_textcnn.txt
          pip install awscli # üõ†Ô∏è Installation de l'AWS CLI
          
      - name: ‚öôÔ∏è V√©rifier la connexion MLflow
        run: python validate_mlflow.py
        
      - name: Prepare data & embeddings (via AWS CLI)
        run: |
          mkdir -p data data/embeddings
          # Dataset
          if [ ! -f "$DATA_CSV" ]; then
            if [ -n "$DATA_CSV_URL" ]; then
              echo "Downloading dataset from $DATA_CSV_URL -> $DATA_CSV"
              # üõ†Ô∏è Remplacement de curl par aws s3 cp
              aws s3 cp "$DATA_CSV_URL" "$DATA_CSV" --region eu-north-1
            else
              echo "No dataset. Skipping job."
              exit 0
            fi
          fi
          # Embeddings
          if [ ! -f "$EMB_PATH" ]; then
            if [ -n "$GLOVE_200D_URL" ]; then
              echo "Downloading GloVe 200d from $GLOVE_200D_URL -> $EMB_PATH"
              mkdir -p "$(dirname "$EMB_PATH")"
              # üõ†Ô∏è Remplacement de curl par aws s3 cp
              aws s3 cp "$GLOVE_200D_URL" "$EMB_PATH" --region eu-north-1
            else
              echo "No embeddings. Skipping job."
              exit 0
            fi
          fi
      - name: Train (smoketest)
        run: |
          python -m scripts.run_textcnn \
            --data "$DATA_CSV" \
            --embedding_path "$EMB_PATH" \
            --embedding_dim 200 \
            --subset_rows 5000 \
            --epochs 2 \
            --batch_size 256

  # --------------------------
  # Baseline LogReg
  # --------------------------
  train-logreg:
    needs: detect-changes
    if: needs.detect-changes.outputs.logreg == 'true' || needs.detect-changes.outputs.common == 'true'
    runs-on: ubuntu-latest
    env:
      MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
      DATA_CSV: data/training.csv
      DATA_CSV_URL: ${{ secrets.DATA_CSV_URL }} # format s3://...
    steps:
      - uses: actions/checkout@v4
      # üîë Configuration des identifiants AWS pour l'acc√®s S3
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: eu-north-1 # R√©gion du bucket S3
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
      - name: Install deps & AWS CLI
        run: |
          python -m pip install -U pip
          pip install -r requirements.txt
          pip install awscli # üõ†Ô∏è Installation de l'AWS CLI
          
      - name: ‚öôÔ∏è V√©rifier la connexion MLflow
        run: python validate_mlflow.py
        
      - name: Prepare dataset (via AWS CLI)
        run: |
          mkdir -p data
          if [ ! -f "$DATA_CSV" ]; then
            if [ -n "$DATA_CSV_URL" ]; then
              echo "Downloading dataset from $DATA_CSV_URL -> $DATA_CSV"
              # üõ†Ô∏è Remplacement de curl par aws s3 cp
              aws s3 cp "$DATA_CSV_URL" "$DATA_CSV" --region eu-north-1
            else
              echo "No dataset. Skipping job."
              exit 0
            fi
          fi
      - name: Valider le Code (Smoketest)
        run: |
          python -m scripts.run_logreg \
            --data "$DATA_CSV" \
            --subset_rows 5000 \
            --ngram_max 2